{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CelebA dataset (both VAE and beta-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plottingGrid(model, test_loader, type):\n",
    "\n",
    "    count = 0\n",
    "    model = model.to('cuda:0')\n",
    "\n",
    "    for batch in test_loader:\n",
    "    # for every batch in test_loader:\n",
    "        count += 1\n",
    "        \n",
    "        if count>1:\n",
    "            break\n",
    "        x, y = batch\n",
    "        x = x.cuda(0)\n",
    "        y = y.cuda(0)\n",
    "        \n",
    "        z = model(x)\n",
    "        x = x.cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(12., 12.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                    nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                    axes_pad=0.1,  # pad between axes in inch.\n",
    "                    )\n",
    "    latte = []\n",
    "    # print(z[0].shape)\n",
    "    for i in range(100):\n",
    "        f = z[0][i].permute(1,2,0)\n",
    "        latte.append(f.cpu().detach().numpy())\n",
    "\n",
    "    for ax, im in zip(grid, latte):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im)\n",
    "\n",
    "    fig.savefig(f'grid_{type}_thirty_epochs.png')\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=256):\n",
    "        return input.view(input.size(0), size, 19, 14)\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, image_channels=3, h_dim=19*14*256, z_dim=32, lr = 1e-3):\n",
    "        self.lr = lr\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=10, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=10, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=5, stride=2, padding = 2, output_padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        std = std.cuda(0)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.cuda(0)\n",
    "        z = mu + std * esp\n",
    "        z = z.cuda(0)\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.cuda(0)\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        z = z.cuda(0)\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        \n",
    "        z = self.fc3(z)\n",
    "        return [self.decoder(z), mu, logvar]\n",
    "    \n",
    "\n",
    "    def loss_fn(self, recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu**2 -  logvar.exp())\n",
    "        return BCE + kl_divergence\n",
    "    \n",
    "    \n",
    "    def loss_function(self,recons,x,mu,logvar):\n",
    "        # Account for the minibatch samples from the dataset; M_N = self.params['batch_size']/ self.num_train_imgs\n",
    "        kld_weight = 0.5\n",
    "        recons_loss =F.mse_loss(recons, x,reduction=\"sum\")\n",
    "        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    counter=0\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        self.counter+=1\n",
    "        x, y = train_batch\n",
    "        x = x.float()\n",
    "        z, mu, logvar = self(x)\n",
    "        loss = self.loss_function(z, x, mu, logvar)\n",
    "        if self.counter % 100 == 0:\n",
    "            print(loss)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class betaVAE(pl.LightningModule):\n",
    "    def __init__(self, image_channels=3, h_dim=19*14*256, z_dim=32, lr = 1e-3, beta = 150):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        super(betaVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=10, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=10, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=5, stride=2, padding = 2, output_padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        std = std.cuda(0)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.cuda(0)\n",
    "        z = mu + std * esp\n",
    "        z = z.cuda(0)\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.cuda(0)\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        z = z.cuda(0)\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        \n",
    "        z = self.fc3(z)\n",
    "        return [self.decoder(z), mu, logvar]    \n",
    "    \n",
    "    def loss_function(self, recons, x, mu, logvar, beta):\n",
    "        # Account for the minibatch samples from the dataset; M_N = self.params['batch_size']/ self.num_train_imgs\n",
    "        kld_weight = 0.5\n",
    "        recons_loss =F.mse_loss(recons, x,reduction=\"sum\")\n",
    "        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "        loss = recons_loss + beta * kld_weight * kld_loss\n",
    "        loss = loss.cuda(0)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    counter=0\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        self.counter+=1\n",
    "        x,y= train_batch\n",
    "        x=x.float()\n",
    "        x = x.cuda(0)\n",
    "        y = y.cuda(0)\n",
    "        z, mu, logvar = self(x)\n",
    "        z = z.cuda(0)\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        loss = self.loss_function(z, x, mu, logvar, self.beta)\n",
    "        loss = loss.cuda(0)\n",
    "        if self.counter%50 ==0:\n",
    "            print(loss)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "count = 0\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "for batch in test_loader:\n",
    "# for batch in test_loader:\n",
    "    count+=1;\n",
    "    # batch = batch.cuda()\n",
    "    \n",
    "    if count>1:\n",
    "        break\n",
    "    print(\"*****************************************************************************************************************\")\n",
    "    x,y = batch\n",
    "    x = x.cuda(0)\n",
    "    y = y.cuda(0)\n",
    "    # print(x.shape)\n",
    "    \n",
    "    z = model(x)\n",
    "#     print(z[0].shape)\n",
    "#     print(type(z))\n",
    "#     print(type(z[0]))\n",
    "    x = x.cpu()\n",
    "    # z = z.cpu()\n",
    "\n",
    "fig = plt.figure(figsize=(12., 12.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "latte = []\n",
    "print(z[0].shape)\n",
    "for i in range(100):\n",
    "    f = z[0][i].permute(1,2,0)\n",
    "    latte.append(f.cpu().detach().numpy())\n",
    "\n",
    "for ax, im in zip(grid, latte):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "# plt.save('grid_vae_ten_epochs.png')\n",
    "fig.savefig('grid_beta_1_vae_thirty_epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # for celeba dataset\n",
    "    data_path = '../data/celeba/img_align_celeba'\n",
    "\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    sets=torch.utils.data.random_split(dataset, [200000, 2599], generator=torch.Generator().manual_seed(2147483647))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        sets[0],\n",
    "        batch_size=128,\n",
    "        num_workers=6,\n",
    "        shuffle=True\n",
    "    )\n",
    "    train_test_loader = torch.utils.data.DataLoader(\n",
    "        sets[0],\n",
    "        batch_size=128,\n",
    "        num_workers=6,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        sets[1],\n",
    "        batch_size=128,\n",
    "        num_workers=6,\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x = x.cuda(0)\n",
    "        y = y.cuda(0)\n",
    "        break\n",
    "\n",
    "    # Vanilla VAE\n",
    "    model = VAE(image_channels=3, z_dim=32, lr =1e-5 )\n",
    "    trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 30, devices = 1, accelerator='gpu')\n",
    "    trainer.fit(model, train_loader) \n",
    "    plottingGrid(model, test_loader, 'vanillaVAE')\n",
    "\n",
    "    # beta VAE\n",
    "    # beta = 1\n",
    "    model1 = betaVAE(image_channels=3, z_dim=32, lr =1e-5, beta = 1)\n",
    "    trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 30, devices = 1, accelerator='gpu')\n",
    "    trainer.fit(model1, train_loader) \n",
    "    plottingGrid(model1, test_loader, 'betaVAE_1')\n",
    "\n",
    "\n",
    "    # beta = 4\n",
    "    model4 = betaVAE(image_channels=3, z_dim=32, lr =1e-5, beta = 4)\n",
    "    trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 30, devices = 1, accelerator='gpu')\n",
    "    trainer.fit(model4, train_loader) \n",
    "    plottingGrid(model4, test_loader, 'betaVAE_4')\n",
    "\n",
    "    # beta = 150\n",
    "    model150 = betaVAE(image_channels=3, z_dim=32, lr =1e-5, beta = 150)\n",
    "    trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 30, devices = 1, accelerator='gpu')\n",
    "    trainer.fit(model150, train_loader) \n",
    "    plottingGrid(model150, test_loader, 'betaVAE_150')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dsprites dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import abc\n",
    "\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, z_dim = 10):\n",
    "        \"\"\"\n",
    "        Model Architecture: \n",
    "        - 4 convolutional layers - each with 32 channels and a kernel size of 4*4\n",
    "        - 2 fc layers - each of 256 unit len\n",
    "        - 1 fc layer for latent distribution of 20 units, that is, mean and log variance for 10 gaussians\n",
    "        \"\"\"\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        #params\n",
    "        hidden_channels = 1\n",
    "        kernel_size = 4\n",
    "        h_dim = 256\n",
    "        self.img_size = img_size\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "        n_channels = self.img_size[0]\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        #fc layers\n",
    "        self.linear1 = nn.Linear(z_dim, h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim, h_dim)\n",
    "        self.linear3 = nn.Linear(h_dim, 64)\n",
    "\n",
    "        # conv layers\n",
    "        kwargs = dict(stride = 2, padding = 1)\n",
    "        self.conv1 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "        self.conv3 = nn.ConvTranspose2d(hidden_channels, n_channels, kernel_size, **kwargs)\n",
    "        self.conv4 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        #fc with relu\n",
    "        x = torch.relu(self.linear1(z))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        x = x.view(batch_size, *(1, 8, 8))\n",
    "\n",
    "        #conv layers with relu\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        # x = torch.relu(self.conv2(x))\n",
    "        # for final layer activation should be sigmoid\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, img_size, z_dim = 10):\n",
    "        \"\"\"\n",
    "        Model Architecture: \n",
    "        - 4 convolutional layers - each with 32 channels and a kernel size of 4*4\n",
    "        - 2 fc layers - each of 256 unit len\n",
    "        - 1 fc layer for latent distribution of 20 units, that is, mean and log variance for 10 gaussians\n",
    "        \"\"\"\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        #params\n",
    "        hidden_channels = 1\n",
    "        kernel_size = 4\n",
    "        h_dim = 256\n",
    "        self.img_size = img_size\n",
    "        self.reshape = (hidden_channels, kernel_size, kernel_size)\n",
    "        n_channels = self.img_size[0]\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # conv layers\n",
    "        kwargs = dict(stride = 2, padding = 1)\n",
    "        self.conv1 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, n_channels, kernel_size, **kwargs)\n",
    "        self.conv4 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size, **kwargs)\n",
    "\n",
    "        #fc layers\n",
    "        self.linear1 = nn.Linear(64, h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "        self.mu_logvar = nn.Linear(h_dim, self.z_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        #conv layers with relu\n",
    "\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        # x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view((batch_size, -1))\n",
    "\n",
    "        #fc layers with relu\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        \n",
    "        # for calculating mean and log variance\n",
    "        mu_logvar = self.mu_logvar(x)\n",
    "        mu, logvar = mu_logvar.view(-1, self.z_dim, 2).unbind(-1)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, img_size, encoder, decoder, z_dim, beta = 1, lr = 1e-4):\n",
    "        \"model and forward pass\"\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.img_size = img_size\n",
    "        self.encoder = encoder(img_size, z_dim)\n",
    "        self.decoder = decoder(img_size, z_dim)\n",
    "        self.beta = beta\n",
    "        self.lr = lr\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        mu = mu.cuda(0)\n",
    "        logvar = logvar.cuda(0)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        std = std.cuda(0)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        esp = esp.cuda(0)\n",
    "        z = mu + std * esp\n",
    "        z = z.cuda(0)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.cuda(0)\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z_sample = self.reparameterize(mu, logvar)\n",
    "        return [self.decoder(z_sample), mu, logvar]    \n",
    "    \n",
    "    def loss_function(self, recons, x, mu, logvar):\n",
    "        # Account for the minibatch samples from the dataset; M_N = self.params['batch_size']/ self.num_train_imgs\n",
    "        kld_weight = 0.5\n",
    "        recons_loss =F.mse_loss(recons, x,reduction=\"sum\")\n",
    "        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "        loss = recons_loss + kld_weight * kld_loss * self.beta\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    counter=0\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        self.counter+=1\n",
    "        x = train_batch\n",
    "        x=x.float()\n",
    "        z, mu, logvar = self(x)\n",
    "        loss = self.loss_function(z, x, mu, logvar)\n",
    "        if self.counter % 100 ==0:\n",
    "            print(loss)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '../data/dsprites'\n",
    "import numpy as np\n",
    "# Load dataset\n",
    "dataset_zip = np.load('../data/dsprites/dsprites_ndarray.npz')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "count = 0\n",
    "# model = model.to('cuda:0')\n",
    "\n",
    "for batch in train_data_loader:\n",
    "# for batch in test_loader:\n",
    "    count+=1    \n",
    "    if count>1:\n",
    "        break\n",
    "    print(\"*****************************************************************************************************************\")\n",
    "    x = batch\n",
    "\n",
    "fig = plt.figure(figsize=(12., 12.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(10, 10),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "latte = []\n",
    "for i in range(100):\n",
    "    f = z[0][i].reshape(64,64)\n",
    "    latte.append(f.cpu().detach().numpy())\n",
    "\n",
    "for ax, im in zip(grid, latte):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im, cmap = 'gray')\n",
    "# plt.save('grid_vae_ten_epochs.png')\n",
    "fig.savefig('dsprites_dataset.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class dspritesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X):\n",
    "        # 'Initialization'\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.X[index]\n",
    "        X = self.transform(image)\n",
    "        return X\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = dspritesDataset(imgs)\n",
    "batch_size = 128\n",
    "sets = torch.utils.data.random_split(dataset, [730000, 7280], generator=torch.Generator().manual_seed(2147483647))\n",
    "train_data_loader = DataLoader(sets[0], batch_size, shuffle = True, num_workers = 3)\n",
    "test_data_loader = DataLoader(sets[1], batch_size, shuffle = True, num_workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (1, 64, 64)\n",
    "model = VAE( img_size, encoder = encoder, decoder = decoder, z_dim = 10, lr = 1e-5 )\n",
    "trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 20, devices = 1, accelerator='gpu')\n",
    "trainer.fit(model, train_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottingGrid(model, train_data_loader, 'VAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = VAE( img_size, encoder = encoder, decoder = decoder, z_dim = 10, lr = 1e-5, beta = 10)\n",
    "trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 10, devices = 1, accelerator='gpu')\n",
    "trainer.fit(model10, train_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottingGrid(model, test_data_loader, 'beta_VAE_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model150 = VAE( img_size, encoder = encoder, decoder = decoder, z_dim = 10, lr = 1e-5, beta = 150 )\n",
    "trainer = pl.Trainer(auto_scale_batch_size=True , max_epochs = 10, devices = 1, accelerator='gpu')\n",
    "trainer.fit(model150, train_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plottingGrid(model, test_data_loader, 'beta_VAE_150')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
